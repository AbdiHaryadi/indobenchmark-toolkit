{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, MBartForConditionalGeneration\n",
    "from src.indobenchmark import IndoNLGTokenizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_param(module, trainable=False):\n",
    "    if trainable:\n",
    "        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in module.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.76 s, sys: 2.03 s, total: 9.79 s\n",
      "Wall time: 15.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gpt_model = GPT2LMHeadModel.from_pretrained('indobenchmark/indogpt')\n",
    "gpt_tokenizer = IndoNLGTokenizer.from_pretrained('indobenchmark/indogpt')\n",
    "\n",
    "bart_model = MBartForConditionalGeneration.from_pretrained('indobenchmark/indobart-v2')\n",
    "bart_tokenizer = IndoNLGTokenizer.from_pretrained('indobenchmark/indobart-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> aku adalah anak pertama dari tiga bersaudara.</s> aku lahir di kota kecil yang sama dengan ayahku.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_input = gpt_tokenizer.prepare_input_for_generation('aku adalah anak', model_type='indogpt', return_tensors='pt')\n",
    "gpt_out = gpt_model.generate(**gpt_input)\n",
    "gpt_tokenizer.decode(gpt_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> aku suka sekali dengan warna-warna yang cerah dan cerah.</s> itu yang membuat aku suka dengan'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_input = gpt_tokenizer.prepare_input_for_generation('aku suka sekali ', model_type='indogpt', return_tensors='pt')\n",
    "gpt_out = gpt_model.generate(**gpt_input)\n",
    "gpt_tokenizer.decode(gpt_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> hai, bagaimana kabar kalian? semoga sehat selalu ya. kali ini saya akan membahas tentang cara membuat'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_input = gpt_tokenizer.prepare_input_for_generation('hai, bagaimana ', model_type='indogpt', return_tensors='pt')\n",
    "gpt_out = gpt_model.generate(**gpt_input)\n",
    "gpt_tokenizer.decode(gpt_out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test BART Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> aku pergi ke toko obat membeli<mask></s>[indonesian]\n",
      "<s> aku pergi ke toko obat membeli obat.[indonesian]\n"
     ]
    }
   ],
   "source": [
    "inputs = ['aku pergi ke toko obat membeli <mask>']\n",
    "bart_input = bart_tokenizer.prepare_input_for_generation(inputs, return_tensors='pt',\n",
    "                                         lang_token = '[indonesian]', decoder_lang_token='[indonesian]')\n",
    "\n",
    "bart_out = bart_model(**bart_input)\n",
    "print(bart_tokenizer.decode(bart_input['input_ids'][0]))\n",
    "print(bart_tokenizer.decode(bart_out.logits.topk(1).indices[:,:].squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> aku menyang pasar<mask></s>[javanese]\n",
      "<s> aku menyang pasar kembang,[javanese]\n"
     ]
    }
   ],
   "source": [
    "inputs = ['aku menyang pasar <mask>']\n",
    "bart_input = bart_tokenizer.prepare_input_for_generation(inputs, return_tensors='pt',\n",
    "                                         lang_token = '[javanese]', decoder_lang_token='[javanese]')\n",
    "\n",
    "bart_out = bart_model(**bart_input)\n",
    "print(bart_tokenizer.decode(bart_input['input_ids'][0]))\n",
    "print(bart_tokenizer.decode(bart_out.logits.topk(1).indices[:,:].squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> kuring ka pasar senen meuli daging<mask></s>[sundanese]\n",
      "<s> kuring ka pasar senen meuli daging sapi,[sundanese]\n"
     ]
    }
   ],
   "source": [
    "inputs = ['kuring ka pasar senen meuli daging <mask>']\n",
    "bart_input = bart_tokenizer.prepare_input_for_generation(inputs, return_tensors='pt',\n",
    "                                         lang_token = '[sundanese]', decoder_lang_token='[sundanese]')\n",
    "\n",
    "bart_out = bart_model(**bart_input)\n",
    "print(bart_tokenizer.decode(bart_input['input_ids'][0]))\n",
    "print(bart_tokenizer.decode(bart_out.logits.topk(1).indices[:,:].squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Batch Loading with Decoder Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_ids': tensor([    0,   528,   450,   646, 21985,     2, 40001]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1]), 'decoder_input_ids': tensor([40000,     0,  1118, 26083,   825,  9131,     2]), 'decoder_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1]), 'labels': tensor([    0,  1118, 26083,   825,  9131,     2, 40000])}, {'input_ids': tensor([    0, 13453,   620,   387,  2402,     2, 40001]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1]), 'decoder_input_ids': tensor([40000,     0, 11934,  4711, 36265, 20667,  4552,  7491,     2]), 'decoder_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor([    0, 11934,  4711, 36265, 20667,  4552,  7491,     2, 40000])}, {'input_ids': tensor([    0,   742,   523,  3097,     2, 40001]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1]), 'decoder_input_ids': tensor([40000,     0, 16544,  5888,     2]), 'decoder_attention_mask': tensor([1, 1, 1, 1, 1]), 'labels': tensor([    0, 16544,  5888,     2, 40000])}]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for enc, dec in zip(\n",
    "    ['aku adalah anak gembala', 'balonku ada lima', 'so I say'], \n",
    "    ['selalu riang serta gembira', 'see you once again my love', 'pokemon master']\n",
    "):\n",
    "    data.append(bart_tokenizer.prepare_input_for_generation(\n",
    "        enc, decoder_inputs=dec, model_type='indobart', return_tensors='pt',\n",
    "        lang_token='[sundanese]', decoder_lang_token='[javanese]', padding=False\n",
    "    ))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   528,   450,   646, 21985,     2, 40001],\n",
      "        [    0, 13453,   620,   387,  2402,     2, 40001],\n",
      "        [    0,   742,   523,  3097,     2, 40001,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0]]), 'decoder_input_ids': tensor([[40000,     0,  1118, 26083,   825,  9131,     2,     1,     1],\n",
      "        [40000,     0, 11934,  4711, 36265, 20667,  4552,  7491,     2],\n",
      "        [40000,     0, 16544,  5888,     2,     1,     1,     1,     1]]), 'decoder_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0]]), 'labels': tensor([[    0,  1118, 26083,   825,  9131,     2, 40000,  -100,  -100],\n",
      "        [    0, 11934,  4711, 36265, 20667,  4552,  7491,     2, 40000],\n",
      "        [    0, 16544,  5888,     2, 40000,  -100,  -100,  -100,  -100]])}\n"
     ]
    }
   ],
   "source": [
    "for batch in DataLoader(data, batch_size=3, collate_fn=lambda t: bart_tokenizer.pad(t, padding='longest')):\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_indonlg",
   "language": "python",
   "name": "env_indonlg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
