{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to load from: IndoNLG_finals_mBart_model_v2_checkpoint_105_640000.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "from transformers import BartModel, BartForConditionalGeneration, GPT2LMHeadModel, MBartForConditionalGeneration, BartConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_for_generation(self, inputs, lang_token = '[indonesia]', decoder_lang_token = '[indonesia]', decoder_inputs=None, return_tensors='pt'):\n",
    "\n",
    "    # Process encoder input\n",
    "    if lang_token not in self.special_tokens_to_ids:\n",
    "        raise ValueError(f\"Unknown lang_token `{lang_token}`, lang_token must be either `[javanese]`, `[sundanese]`, or `[indonesian]`\")  \n",
    "    elif type(inputs) == list:\n",
    "        if len(inputs) == 0 or type(inputs[0]) != str:\n",
    "            raise ValueError(IndoNLGTokenizer.input_error_message)\n",
    "    elif type(inputs) != str:\n",
    "        raise ValueError(IndoNLGTokenizer.input_error_message)\n",
    "\n",
    "    lang_id = self.special_tokens_to_ids[lang_token]\n",
    "    input_batch = self(inputs, return_attention_mask=False)\n",
    "    input_batch['input_ids'][0] = input_batch['input_ids'][0][1:-1]\n",
    "    print(input_batch)\n",
    "    if type(inputs) == str:\n",
    "        input_batch['input_ids'] = [self.bos_token_id] + input_batch['input_ids'] + [self.eos_token_id, lang_id]\n",
    "    else:\n",
    "        input_batch['input_ids'] = list(map(\n",
    "            lambda input_ids: [self.bos_token_id] + input_ids + [self.eos_token_id, lang_id], \n",
    "            input_batch['input_ids']))\n",
    "\n",
    "    if decoder_inputs is None:\n",
    "        # Return encoder input\n",
    "        return self.pad(input_batch, return_tensors=return_tensors)\n",
    "    else:\n",
    "        # Process decoder input\n",
    "        if decoder_lang_token not in self.special_tokens_to_ids:\n",
    "            raise ValueError(f\"Unknown decoder_lang_token `{decoder_lang_token}`, decoder_lang_token must be either `[javanese]`, `[sundanese]`, or `[indonesian]`\")  \n",
    "        elif type(decoder_inputs) == list:\n",
    "            if len(decoder_inputs) == 0:\n",
    "                raise ValueError(IndoNLGTokenizer.input_error_message)\n",
    "            elif type(decoder_inputs[0]) != str:\n",
    "                raise ValueError(IndoNLGTokenizer.input_error_message)\n",
    "        elif type(decoder_inputs) != str:\n",
    "            raise ValueError(IndoNLGTokenizer.input_error_message)\n",
    "\n",
    "        decoder_lang_id = self.special_tokens_to_ids[decoder_lang_token]\n",
    "        decoder_input_batch = self(decoder_inputs, return_attention_mask=False)\n",
    "        decoder_input_batch['input_ids'][0] = decoder_input_batch['input_ids'][0][1:-1]\n",
    "\n",
    "        if type(decoder_inputs) == str:\n",
    "            decoder_input_batch['input_ids'] = [lang_id, self.bos_token_id] + decoder_input_batch['input_ids']  + [self.eos_token_id]\n",
    "        else:\n",
    "            decoder_input_batch['input_ids'] = list(map(lambda input_ids: [lang_id, self.bos_token_id] + input_ids + [self.eos_token_id], decoder_input_batch['input_ids']))\n",
    "\n",
    "        # Padding\n",
    "        input_batch = self.pad(input_batch, return_tensors=return_tensors)\n",
    "        decoder_input_batch = self.pad(decoder_input_batch, return_tensors=return_tensors)\n",
    "\n",
    "        # Store into a single dict\n",
    "        input_batch['decoder_input_ids'] = decoder_input_batch['input_ids']\n",
    "        input_batch['decoder_attention_mask'] = decoder_input_batch['attention_mask']\n",
    "\n",
    "        return input_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = '/home/samuel/indonlg/checkpoints/IndoNLG_finals_mBart_model_v2_checkpoint_105_640000.pt'\n",
    "vocab_path = 'IndoNLG_finals_vocab_model_indo4b_plus_spm_bpe_9995_wolangid_bos_pad_eos_unk.model'\n",
    "\n",
    "# source_lang = \"id_ID\"\n",
    "# target_lang = \"su_SU\"\n",
    "\n",
    "config = BartConfig.from_pretrained('facebook/bart-base')\n",
    "config.vocab_size = 40004\n",
    "model = MBartForConditionalGeneration(config=config)\n",
    "\n",
    "bart = BartModel(config=config)\n",
    "bart.load_state_dict(torch.load(model_checkpoint)['model'], strict=False)\n",
    "# bart.shared.weight = bart.encoder.embed_tokens.weight\n",
    "model.model = bart\n",
    "\n",
    "bart_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[498, 410, 1859, 1035, 39942, 39995]]}\n"
     ]
    }
   ],
   "source": [
    "from tokenization_indonlg import IndoNLGTokenizer\n",
    "\n",
    "tokenizer = IndoNLGTokenizer(vocab_file=vocab_path)\n",
    "\n",
    "inputs = ['aku adalah <mask>']\n",
    "bart_input = prepare_input_for_generation(tokenizer, inputs, decoder_inputs=['aku adalah <mask>'], return_tensors='pt')\n",
    "\n",
    "# bart_out = bart_model(**bart_input)\n",
    "# print(bart_input)\n",
    "# tokenizer.decode(bart_out.logits.topk(1).indices[:,:,:].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   498,   410,  1859,  1035, 39942, 39995,     2, 40002]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'decoder_input_ids': tensor([[40002,     0,   498,   410,  1859,  1035, 39942, 39995,     2]]), 'decoder_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'para sudah saat? amp<0x4C> 19 paraentu'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_out = bart(**bart_input)\n",
    "print(bart_input)\n",
    "tokenizer.decode(bart_out.last_hidden_state.topk(1).indices[:,:,:].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2270],\n",
      "         [11944],\n",
      "         [25506],\n",
      "         [39969],\n",
      "         [ 5130],\n",
      "         [19261],\n",
      "         [20773],\n",
      "         [ 5130],\n",
      "         [11944]]]) bermanirat mengasuh5 dénebur penandatanganan dénirat tensor(6.4212e-05, grad_fn=<MeanBackward0>)\n",
      "tensor([[[30675],\n",
      "         [ 7838],\n",
      "         [16819],\n",
      "         [12175],\n",
      "         [ 2270],\n",
      "         [ 8933],\n",
      "         [20773],\n",
      "         [ 6739],\n",
      "         [30675]]]) urban pdf dulur ingatlah berman bebentukan penandatanganan riwayaturban tensor(6.4212e-05, grad_fn=<MeanBackward0>)\n",
      "tensor([[[27608],\n",
      "         [33025],\n",
      "         [ 4403],\n",
      "         [ 5130],\n",
      "         [27970],\n",
      "         [ 5130],\n",
      "         [20773],\n",
      "         [22296],\n",
      "         [33273]]]) memanaskanotype yuk dén kolon dén penandatanganan iyo zhou tensor(6.4212e-05, grad_fn=<MeanBackward0>)\n",
      "tensor([[[25486],\n",
      "         [ 2270],\n",
      "         [23155],\n",
      "         [23155],\n",
      "         [ 5130],\n",
      "         [ 5682],\n",
      "         [ 7088],\n",
      "         [21319],\n",
      "         [21255]]]) buton berman berair berair dén baiklah mempercepat disematkan diwarnai tensor(6.4212e-05, grad_fn=<MeanBackward0>)\n",
      "tensor([[[39969],\n",
      "         [21319],\n",
      "         [39969],\n",
      "         [39969],\n",
      "         [25519],\n",
      "         [23155],\n",
      "         [10880],\n",
      "         [ 5130],\n",
      "         [39969]]]) 5 disematkan55 progo berairembak dén5 tensor(6.4212e-05, grad_fn=<MeanBackward0>)\n",
      "tensor([[[19261],\n",
      "         [ 1647],\n",
      "         [39969],\n",
      "         [19813],\n",
      "         [37335],\n",
      "         [19261],\n",
      "         [ 6275],\n",
      "         [21319],\n",
      "         [39969]]]) ebur menemukan5 mengutus videosebur mangg disematkan5 tensor(6.4212e-05, grad_fn=<MeanBackward0>)\n",
      "tensor([[[14064],\n",
      "         [18902],\n",
      "         [34157],\n",
      "         [ 5130],\n",
      "         [15029],\n",
      "         [19261],\n",
      "         [34157],\n",
      "         [ 3161],\n",
      "         [32650]]]) heart membelahrefour dén pacarnyaeburrefourapus rally tensor(6.4212e-05, grad_fn=<MeanBackward0>)\n",
      "tensor([[[ 6275],\n",
      "         [16257],\n",
      "         [30547],\n",
      "         [11689],\n",
      "         [ 5130],\n",
      "         [ 5130],\n",
      "         [12184],\n",
      "         [ 5130],\n",
      "         [39471]]]) mangg yul sanalah fleksibel dén dén deli dén memanggang tensor(6.4212e-05, grad_fn=<MeanBackward0>)\n",
      "tensor([[[ 8468],\n",
      "         [ 5130],\n",
      "         [ 4121],\n",
      "         [ 5130],\n",
      "         [ 5130],\n",
      "         [37591],\n",
      "         [ 5130],\n",
      "         [ 5130],\n",
      "         [ 5130]]]) atar dén cewek dén dén brc dén dén dén tensor(6.4212e-05, grad_fn=<MeanBackward0>)\n",
      "tensor([[[29459],\n",
      "         [28562],\n",
      "         [37661],\n",
      "         [39969],\n",
      "         [39969],\n",
      "         [ 5682],\n",
      "         [33179],\n",
      "         [ 5130],\n",
      "         [30675]]]) pelihara btc rtr55 baiklah chamber dénurban tensor(6.4212e-05, grad_fn=<MeanBackward0>)\n",
      "tensor([[[ 5130],\n",
      "         [16819],\n",
      "         [ 5130],\n",
      "         [19813],\n",
      "         [ 5130],\n",
      "         [35755],\n",
      "         [26035],\n",
      "         [25767],\n",
      "         [ 5130]]]) dén dulur dén mengutus dén nira dirut eyeliner dén tensor(6.4212e-05, grad_fn=<MeanBackward0>)\n",
      "tensor([[[39969],\n",
      "         [30675],\n",
      "         [39969],\n",
      "         [ 5130],\n",
      "         [39969],\n",
      "         [19261],\n",
      "         [20773],\n",
      "         [ 5130],\n",
      "         [ 5130]]]) 5urban5 dén5ebur penandatanganan dén dén tensor(6.4212e-05, grad_fn=<MeanBackward0>)\n",
      "tensor([[[ 2270],\n",
      "         [ 2270],\n",
      "         [10526],\n",
      "         [23549],\n",
      "         [39969],\n",
      "         [ 4797],\n",
      "         [27003],\n",
      "         [ 5130],\n",
      "         [ 5130]]]) berman bermantek normalnya5 uurob dén dén tensor(6.4212e-05, grad_fn=<MeanBackward0>)\n",
      "tensor([[[39969],\n",
      "         [22938],\n",
      "         [39969],\n",
      "         [13369],\n",
      "         [ 5130],\n",
      "         [ 5130],\n",
      "         [20773],\n",
      "         [ 5130],\n",
      "         [ 5130]]]) 5thing5 kepri dén dén penandatanganan dén dén tensor(6.4212e-05, grad_fn=<MeanBackward0>)\n",
      "tensor([[[22074],\n",
      "         [34073],\n",
      "         [12405],\n",
      "         [31155],\n",
      "         [27970],\n",
      "         [31155],\n",
      "         [13718],\n",
      "         [33423],\n",
      "         [11833]]]) ngawangun menggoyang peluru phase kolon phase penegak evolution cemb tensor(6.4212e-05, grad_fn=<MeanBackward0>)\n",
      "tensor([[[ 5130],\n",
      "         [27968],\n",
      "         [ 4326],\n",
      "         [ 5130],\n",
      "         [ 5130],\n",
      "         [33107],\n",
      "         [15915],\n",
      "         [ 5130],\n",
      "         [27822]]]) dénmont keur dén dén airbusimin dén nepikeun tensor(6.4212e-05, grad_fn=<MeanBackward0>)\n",
      "tensor([[[15314],\n",
      "         [11944],\n",
      "         [34601],\n",
      "         [ 5588],\n",
      "         [27970],\n",
      "         [34225],\n",
      "         [34157],\n",
      "         [23318],\n",
      "         [ 6996]]]) kaulinanirat menyekolahkanetulan kolon kalkulasirefourohaals tensor(6.4212e-05, grad_fn=<MeanBackward0>)\n",
      "tensor([[[13554],\n",
      "         [19261],\n",
      "         [39969],\n",
      "         [39969],\n",
      "         [39969],\n",
      "         [19261],\n",
      "         [20773],\n",
      "         [19261],\n",
      "         [ 5130]]]) kacauebur555ebur penandatangananebur dén tensor(6.4212e-05, grad_fn=<MeanBackward0>)\n",
      "tensor([[[ 2270],\n",
      "         [31527],\n",
      "         [30675],\n",
      "         [ 5130],\n",
      "         [ 2270],\n",
      "         [19261],\n",
      "         [19595],\n",
      "         [30675],\n",
      "         [30675]]]) berman minhourban dén bermanebur disewakanurbanurban tensor(6.4212e-05, grad_fn=<MeanBackward0>)\n",
      "tensor([[[23749],\n",
      "         [ 4873],\n",
      "         [30547],\n",
      "         [ 8468],\n",
      "         [39969],\n",
      "         [18341],\n",
      "         [ 5130],\n",
      "         [17541],\n",
      "         [ 5130]]]) eberkan sabar sanalahatar5 manja dénegradasi dén tensor(6.4212e-05, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    bart_out = bart_model(**bart_input)\n",
    "    print(bart_out.logits.topk(1).indices, tokenizer.decode(bart_out.logits.topk(1).indices[:,:,:].squeeze()), bart_model.model.encoder.embed_tokens.weight.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_out.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(bart_input['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(bart_input['decoder_input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.indobenchmark import IndoNLGTokenizer\n",
    "\n",
    "# tokenizer = IndoNLGTokenizer.from_pretrained('indobenchmark/indobart')\n",
    "# bart_input = tokenizer.prepare_input_for_generation(['aku adalah <mask>'], model_type='indobart', return_tensors='pt')\n",
    "# bart_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_input = tokenizer.prepare_input_for_generation(['aku adalah <mask>'], model_type='indobart', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_input = tokenizer.prepare_input_for_generation(['abdi teh ayeuna','abdi teh ayeuna'], lang_token='[indonesian]',\n",
    "    decoder_inputs=['abdi teh ayeuna','abdi teh ayeuna'], decoder_lang_token='[indonesian]', model_type='indobart', return_tensors='pt')\n",
    "bart_out = bart_model(**bart_input)\n",
    "tokenizer.decode(bart_out.logits.topk(1).indices[0,:,:].squeeze()), tokenizer.decode(bart_out.logits.topk(1).indices[1,:,:].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_param(module, trainable=False):\n",
    "    if trainable:\n",
    "        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in module.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bart_model = BartForConditionalGeneration.from_pretrained('indobenchmark/indobart')\n",
    "# gpt_model = GPT2LMHeadModel.from_pretrained('indobenchmark/indogpt')\n",
    "tokenizer = IndoNLGTokenizer.from_pretrained('indobenchmark/indobart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_input = tokenizer.prepare_input_for_generation('aku adalah anak', model_type='indogpt', return_tensors='pt')\n",
    "gpt_out = gpt_model.generate(**gpt_input)\n",
    "tokenizer.decode(gpt_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_input = tokenizer.prepare_input_for_generation('aku suka sekali makan', model_type='indogpt', return_tensors='pt')\n",
    "gpt_out = gpt_model.generate(**gpt_input)\n",
    "tokenizer.decode(gpt_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_input = tokenizer.prepare_input_for_generation('hai, bagaimana kabar', model_type='indogpt', return_tensors='pt')\n",
    "gpt_out = gpt_model.generate(**gpt_input)\n",
    "tokenizer.decode(gpt_out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test BART Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_input = tokenizer.prepare_input_for_generation(['aku adalah <mask>'], model_type='indobart', return_tensors='pt')\n",
    "bart_out = bart_model(**bart_input)\n",
    "tokenizer.decode(bart_out.logits.topk(1).indices[:,:,:].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_input = tokenizer.prepare_input_for_generation(['abdi teh ayeuna','abdi teh ayeuna'], lang_token='[indonesian]',\n",
    "    decoder_inputs=['abdi teh ayeuna','abdi teh ayeuna'], decoder_lang_token='[indonesian]', model_type='indobart', return_tensors='pt')\n",
    "bart_out = bart_model(**bart_input)\n",
    "tokenizer.decode(bart_out.logits.topk(1).indices[0,:,:].squeeze()), tokenizer.decode(bart_out.logits.topk(1).indices[1,:,:].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_indonlg)",
   "language": "python",
   "name": "env_indonlg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
